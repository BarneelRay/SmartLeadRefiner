{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up environment"
      ],
      "metadata": {
        "id": "OhA7ExCvBwDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "enRC6xLwBMfw"
      },
      "outputs": [],
      "source": [
        "!mkdir -p app data report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai beautifulsoup4 pandas streamlit scraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XvIwoJjBV5w",
        "outputId": "f0880397-eb80-425a-bdf0-99ef3be9a513"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.91.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.1)\n",
            "Requirement already satisfied: scraper in /usr/local/lib/python3.11/dist-packages (0.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: lxml>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from scraper) (5.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.44.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape crunchbase sample"
      ],
      "metadata": {
        "id": "BoTeKFluCw9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# smart_lead_refiner/app/scraper.py\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load mock data from JSON file\n",
        "MOCK_FILE = \"data/mock_company_data.json\"\n",
        "\n",
        "def load_mock_data():\n",
        "    with open(MOCK_FILE, \"r\") as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Clearbit or mock enrichment function\n",
        "def enrich_company(domain, api_key):\n",
        "    if os.environ.get(\"USE_MOCK_DATA\") == \"1\":\n",
        "        mock_db = load_mock_data()\n",
        "        return mock_db.get(domain, None)\n",
        "\n",
        "    url = f\"https://company.clearbit.com/v2/companies/find?domain={domain}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Wrapper to build DataFrame from list of domains\n",
        "def scrape_via_clearbit(domains, api_key=\"mock\"):\n",
        "    data = []\n",
        "    for domain in domains:\n",
        "        company = enrich_company(domain, api_key)\n",
        "        if company:\n",
        "            data.append({\n",
        "                \"Company Name\": company.get(\"name\"),\n",
        "                \"Website\": company.get(\"domain\"),\n",
        "                \"Industry\": company.get(\"category\", {}).get(\"industry\", \"N/A\"),\n",
        "                \"Team Size\": company.get(\"metrics\", {}).get(\"employees\", \"N/A\"),\n",
        "                \"Funding\": company.get(\"metrics\", {}).get(\"raised\", \"N/A\"),\n",
        "                \"Location\": company.get(\"location\", \"N/A\")\n",
        "            })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    os.environ[\"USE_MOCK_DATA\"] = \"1\"\n",
        "    domains = [\"openai.com\", \"notion.so\", \"zapier.com\", \"loom.com\", \"figma.com\"]\n",
        "    df = scrape_via_clearbit(domains)\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YyjBV2gBYHO",
        "outputId": "247cf85e-b287-4a6f-f6bb-5295bc3849db"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Company Name     Website               Industry Team Size Funding  \\\n",
            "0       OpenAI  openai.com            AI Research     >1000   $11B+   \n",
            "1       Notion   notion.so  Productivity Software   200-500   $350M   \n",
            "2       Zapier  zapier.com        Automation SaaS  500-1000   $1.3B   \n",
            "3         Loom    loom.com        Video Messaging   100-200   $203M   \n",
            "4        Figma   figma.com   Design Collaboration  500-1000   $332M   \n",
            "\n",
            "            Location  \n",
            "0  San Francisco, CA  \n",
            "1  San Francisco, CA  \n",
            "2             Remote  \n",
            "3  San Francisco, CA  \n",
            "4  San Francisco, CA  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lead scoring"
      ],
      "metadata": {
        "id": "EtfvhU_SC4KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_lead(row):\n",
        "    score = 0\n",
        "\n",
        "    # Funding weight\n",
        "    try:\n",
        "        if isinstance(row[\"Funding\"], str) and \"$\" in row[\"Funding\"]:\n",
        "            amount = float(row[\"Funding\"].replace(\"$\", \"\").replace(\"B\", \"e9\").replace(\"M\", \"e6\").replace(\"+\", \"\"))\n",
        "            score += amount / 1e8\n",
        "        elif isinstance(row[\"Funding\"], (int, float)):\n",
        "            score += row[\"Funding\"] / 1e8\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Team size weight\n",
        "    try:\n",
        "        if isinstance(row[\"Team Size\"], str):\n",
        "            size = int(str(row[\"Team Size\"]).split(\"-\")[0].replace(\">\", \"\"))\n",
        "            if size >= 1000:\n",
        "                score += 10\n",
        "            elif size >= 500:\n",
        "                score += 8\n",
        "            elif size >= 200:\n",
        "                score += 6\n",
        "        elif isinstance(row[\"Team Size\"], (int, float)):\n",
        "            size = row[\"Team Size\"]\n",
        "            if size >= 1000:\n",
        "                score += 10\n",
        "            elif size >= 500:\n",
        "                score += 8\n",
        "            elif size >= 200:\n",
        "                score += 6\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Industry relevance\n",
        "    if isinstance(row[\"Industry\"], str) and any(keyword in row[\"Industry\"].lower() for keyword in [\"ai\", \"automation\", \"software\"]):\n",
        "        score += 5\n",
        "\n",
        "    return round(score, 2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from app.scraper import scrape_via_clearbit\n",
        "    api_key = \"sk_your_clearbit_key_here\"\n",
        "    df = scrape_via_clearbit([\"openai.com\", \"notion.so\"], api_key)\n",
        "    df[\"Score\"] = df.apply(score_lead, axis=1)\n",
        "    print(df.sort_values(\"Score\", ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "xJ2r-7EhCZu7",
        "outputId": "98593b8e-28c6-4e4d-9e6b-cf0bb841b7a5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'app.scraper'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-2693813974.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscraper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscrape_via_clearbit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sk_your_clearbit_key_here\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_via_clearbit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"openai.com\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"notion.so\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'app.scraper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#static email generator"
      ],
      "metadata": {
        "id": "fQfmrH1tDAjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_email(company_name, industry, pain_point, contact_name=\"there\"):\n",
        "    email_template = f\"\"\"\n",
        "    Hi {contact_name},\n",
        "\n",
        "    I came across {company_name} and was really impressed by your work in the {industry} space.\n",
        "\n",
        "    I've worked with a number of growing companies facing similar challenges around {pain_point}. Based on what I've seen, I believe there's real potential to optimize operations and accelerate growth with a bit of smart tooling.\n",
        "\n",
        "    Would love to share a few ideas — totally no pressure. Let me know if a quick 15-minute chat next week works for you.\n",
        "\n",
        "    Best,\n",
        "    Barneel Ray\n",
        "    AI & Automation Enthusiast | Robotics Engineer\n",
        "    \"\"\"\n",
        "    return email_template.strip()\n",
        "\n",
        "# Example:\n",
        "if __name__ == \"__main__\":\n",
        "    print(generate_email(\"Notion\", \"Productivity Software\", \"managing cross-functional remote teams\", \"Akshay\"))\n"
      ],
      "metadata": {
        "id": "QDD2Uxj1C958"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline execution"
      ],
      "metadata": {
        "id": "-TLbVwuoDHIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from app.scraper import scrape_via_clearbit\n",
        "from app.lead_scorer import score_lead\n",
        "from app.email_generator import generate_email\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Provide your API Key and domains\n",
        "api_key = \"sk_your_clearbit_key_here\"\n",
        "domains = [\"openai.com\", \"notion.so\", \"zapier.com\"]\n",
        "\n",
        "# Step 2: Enrich data\n",
        "df = scrape_via_clearbit(domains, api_key)\n",
        "\n",
        "# Step 3: Score leads\n",
        "df[\"Score\"] = df.apply(score_lead, axis=1)\n",
        "\n",
        "# Step 4: Add pain points + generate emails\n",
        "pain_point_map = {\n",
        "    \"AI Research\": \"scaling AI model deployment and compliance\",\n",
        "    \"Productivity Software\": \"managing cross-functional remote teams\",\n",
        "    \"Automation SaaS\": \"workflow fragmentation and integration bottlenecks\"\n",
        "}\n",
        "\n",
        "def get_pain_point(industry):\n",
        "    return pain_point_map.get(industry, \"growth challenges in tech sectors\")\n",
        "\n",
        "df[\"Pain Point\"] = df[\"Industry\"].apply(get_pain_point)\n",
        "df[\"Email\"] = df.apply(lambda row: generate_email(row[\"Company Name\"], row[\"Industry\"], row[\"Pain Point\"]), axis=1)\n",
        "\n",
        "# Step 5: Sort + Save\n",
        "sorted_df = df.sort_values(\"Score\", ascending=False)\n",
        "sorted_df.to_csv(\"data/leads_scored.csv\", index=False)\n",
        "\n",
        "print(\"\\n--- Pipeline Complete: Top Leads ---\\n\")\n",
        "print(sorted_df[[\"Company Name\", \"Score\"]])"
      ],
      "metadata": {
        "id": "k78-f8lBDKIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}